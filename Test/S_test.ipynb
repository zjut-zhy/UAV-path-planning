{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "443ee9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "工作目录已改为: /Users/zhy/Documents/GitHub/UAV-path-planning\n",
      "已添加路径: /Users/zhy/Documents/GitHub/UAV-path-planning/Single UAV path planning/path planning\n",
      "✓ 路径验证成功\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# 改变工作目录到上一级 (项目根目录)\n",
    "if os.path.basename(os.getcwd()) == 'Test':\n",
    "    os.chdir('..')\n",
    "    print(f\"工作目录已改为: {os.getcwd()}\")\n",
    "\n",
    "# 添加路径到sys.path\n",
    "uav_path = os.path.join(os.getcwd(), 'Single UAV path planning', 'path planning')\n",
    "if uav_path not in sys.path:\n",
    "    sys.path.append(uav_path)\n",
    "    print(f\"已添加路径: {uav_path}\")\n",
    "\n",
    "# 验证路径\n",
    "if os.path.exists(uav_path):\n",
    "    print(\"✓ 路径验证成功\")\n",
    "else:\n",
    "    print(\"✗ 路径验证失败\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ee1962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机测试中...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'quit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 129\u001b[39m\n\u001b[32m     73\u001b[39m     \u001b[38;5;66;03m# env.close()\u001b[39;00m\n\u001b[32m     74\u001b[39m     \u001b[38;5;66;03m# d = {\"all_ep_V\": all_ep_V, \"all_ep_U\": all_ep_U, \"all_ep_T\": all_ep_T}\u001b[39;00m\n\u001b[32m     75\u001b[39m     \u001b[38;5;66;03m# f = open(shoplistfile_test, 'wb')  # 二进制打开，如果找不到该文件，则创建一个\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    126\u001b[39m     \u001b[38;5;66;03m# plt.legend()\u001b[39;00m\n\u001b[32m    127\u001b[39m     \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m():\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(env)\u001b[39m\n\u001b[32m     57\u001b[39m state = new_state\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RENDER:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/UAV-path-planning/Single UAV path planning/path planning/rl_env/path_env.py:279\u001b[39m, in \u001b[36mRlGame.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event.type == pygame.QUIT:\n\u001b[32m    278\u001b[39m     pygame.display.quit()\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[43mquit\u001b[49m()\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m event.type == pygame.MOUSEMOTION:\n\u001b[32m    281\u001b[39m     \u001b[38;5;28mself\u001b[39m.mouse_pos = pygame.mouse.get_pos()\n",
      "\u001b[31mNameError\u001b[39m: name 'quit' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#开发者：Bright Fang\n",
    "#开发时间：2023/7/30 18:13\n",
    "from rl_env.path_env import RlGame\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import pickle as pkl\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "shoplistfile_test = 'Multi-UAVs path planning/path planning/sample_indextest'  #保存文件数据所在文件的文件名'\n",
    "N_Agent=1\n",
    "M_Enemy=5\n",
    "L_Obstacle=3\n",
    "RENDER=True\n",
    "# RENDER=False\n",
    "env = RlGame(n=N_Agent,m=M_Enemy,l=L_Obstacle,render=RENDER).unwrapped\n",
    "EPIOSDE_ALL=500\n",
    "TEST_EPIOSDE=100\n",
    "TRAIN_NUM = 5\n",
    "EP_LEN = 1000\n",
    "state_number=7\n",
    "action_number=env.action_space.shape[0]\n",
    "max_action = env.action_space.high[0]\n",
    "min_action = env.action_space.low[0]\n",
    "LR_A = 5e-4    # learning rate for actor\n",
    "LR_C = 1e-3    # learning rate for critic\n",
    "GAMMA = 0.9\n",
    "MemoryCapacity=20000\n",
    "Batch=128\n",
    "Switch=1\n",
    "tau = 0.0005\n",
    "\n",
    "def main():\n",
    "    run(env)\n",
    "def run(env):\n",
    "    print('随机测试中...')\n",
    "    win_times = 0\n",
    "    average_timestep=0\n",
    "    average_integral_V=0\n",
    "    average_integral_U= 0\n",
    "    all_ep_V, all_ep_U, all_ep_T = [], [], []\n",
    "    for j in range(TEST_EPIOSDE):\n",
    "        state = env.reset()\n",
    "        total_rewards = 0\n",
    "        integral_V=0\n",
    "        integral_U=0\n",
    "        for timestep in range(EP_LEN):\n",
    "            for i in range(N_Agent+M_Enemy):\n",
    "                action = env.action_space.sample()\n",
    "            new_state, reward,done,edge_r,obstacle_r,goal_r,win= env.step(action)  # 执行动作\n",
    "            if win:\n",
    "                win_times += 1\n",
    "            integral_V += state[2]\n",
    "            integral_U += abs(action).sum()\n",
    "            total_rewards += reward\n",
    "            state = new_state\n",
    "            if RENDER:\n",
    "                env.render()\n",
    "            if done:\n",
    "                break\n",
    "        average_timestep += timestep\n",
    "        average_integral_V += integral_V\n",
    "        average_integral_U += integral_U\n",
    "        print(\"Score\", total_rewards)\n",
    "        all_ep_V.append(integral_V)\n",
    "        all_ep_U.append(integral_U)\n",
    "        all_ep_T.append(timestep)\n",
    "    print('任务完成率', win_times / TEST_EPIOSDE)\n",
    "    print('平均最短飞行时间', average_timestep / TEST_EPIOSDE)\n",
    "    print('平均最短飞行路程', average_integral_V / TEST_EPIOSDE)\n",
    "    print('平均最小能量损耗', average_integral_U / TEST_EPIOSDE)\n",
    "    # env.close()\n",
    "    # d = {\"all_ep_V\": all_ep_V, \"all_ep_U\": all_ep_U, \"all_ep_T\": all_ep_T}\n",
    "    # f = open(shoplistfile_test, 'wb')  # 二进制打开，如果找不到该文件，则创建一个\n",
    "    # pkl.dump(d, f, pkl.HIGHEST_PROTOCOL)  # 写入文件\n",
    "    # f.close()\n",
    "\n",
    "    # win_times = 0\n",
    "    # average_timestep=0\n",
    "    # average_integral_V=0\n",
    "    # average_integral_U= 0\n",
    "    # all_ep_r = [[] for i in range(TRAIN_NUM)]\n",
    "    # for k in range(TRAIN_NUM):\n",
    "    #     for j in range(TEST_EPIOSDE):\n",
    "    #         state = env.reset()\n",
    "    #         total_rewards = 0\n",
    "    #         integral_V=0\n",
    "    #         integral_U=0\n",
    "    #         for timestep in range(EP_LEN):\n",
    "    #             for i in range(N_Agent+M_Enemy):\n",
    "    #                 action = env.action_space.sample()\n",
    "    #             new_state, reward,done,edge_r,obstacle_r,goal_r,win= env.step(action)  # 执行动作\n",
    "    #             if win:\n",
    "    #                 win_times += 1\n",
    "    #             integral_V += state[2]\n",
    "    #             integral_U += abs(action).sum()\n",
    "    #             total_rewards += reward\n",
    "    #             state = new_state\n",
    "    #             if RENDER:\n",
    "    #                 env.render()\n",
    "    #             if done:\n",
    "    #                 break\n",
    "    #         average_timestep += timestep\n",
    "    #         average_integral_V += integral_V\n",
    "    #         average_integral_U += integral_U\n",
    "    #         print(\"Score\", total_rewards)\n",
    "    #         all_ep_r[k].append(total_rewards)\n",
    "    #     print('任务完成率', win_times / TEST_EPIOSDE)\n",
    "    #     print('平均最短飞行时间', average_timestep / TEST_EPIOSDE)\n",
    "    #     print('平均最短飞行路程', average_integral_V / TEST_EPIOSDE)\n",
    "    #     print('平均最小能量损耗', average_integral_U / TEST_EPIOSDE)\n",
    "    #     # env.close()\n",
    "    # all_ep_r_mean = np.mean((np.array(all_ep_r)), axis=0)\n",
    "    # all_ep_r_std = np.std((np.array(all_ep_r)), axis=0)\n",
    "    # d = {\"all_ep_r_mean\": all_ep_r_mean, \"all_ep_r_std\": all_ep_r_std}\n",
    "    # f = open(shoplistfile_test, 'wb')  # 二进制打开，如果找不到该文件，则创建一个\n",
    "    # pkl.dump(d, f, pkl.HIGHEST_PROTOCOL)  # 写入文件\n",
    "    # f.close()\n",
    "    # all_ep_r_max = all_ep_r_mean + all_ep_r_std * 0.95\n",
    "    # all_ep_r_min = all_ep_r_mean - all_ep_r_std * 0.95\n",
    "    # plt.plot(np.arange(len(all_ep_r_mean)), all_ep_r_mean, label='随机策略', color='#e75840')\n",
    "    # plt.fill_between(np.arange(len(all_ep_r_mean)), all_ep_r_max, all_ep_r_min, alpha=0.6, facecolor='#e75840')\n",
    "    # plt.xlabel('Monte Carlo测试回合数')\n",
    "    # plt.ylabel('总奖励')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66ad3f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始Go-Explore专家数据生成...\n",
      "阶段1: 初始随机探索\n",
      "初始探索进度: 0/100, 发现网格数: 3\n",
      "初始探索进度: 20/100, 发现网格数: 11\n",
      "gg\n",
      "初始探索进度: 40/100, 发现网格数: 11\n",
      "gg\n",
      "初始探索进度: 60/100, 发现网格数: 12\n",
      "gg\n",
      "初始探索进度: 80/100, 发现网格数: 12\n",
      "gg\n",
      "gg\n",
      "阶段2: 基于存档的系统探索\n",
      "探索进度: 100/2000\n",
      "发现网格数: 12\n",
      "成功轨迹数: 0\n",
      "最佳奖励: -0.22\n",
      "专家轨迹数: 0\n",
      "--------------------------------------------------\n",
      "探索进度: 200/2000\n",
      "发现网格数: 12\n",
      "成功轨迹数: 0\n",
      "最佳奖励: -0.22\n",
      "专家轨迹数: 0\n",
      "--------------------------------------------------\n",
      "gg\n",
      "gg\n",
      "gg\n",
      "gg\n",
      "gg\n",
      "探索进度: 300/2000\n",
      "发现网格数: 12\n",
      "成功轨迹数: 0\n",
      "最佳奖励: -0.21\n",
      "专家轨迹数: 0\n",
      "--------------------------------------------------\n",
      "gg\n",
      "gg\n",
      "gg\n",
      "探索进度: 400/2000\n",
      "发现网格数: 12\n",
      "成功轨迹数: 0\n",
      "最佳奖励: -0.21\n",
      "专家轨迹数: 0\n",
      "--------------------------------------------------\n",
      "gg\n",
      "gg\n",
      "gg\n",
      "gg\n",
      "gg\n",
      "探索进度: 500/2000\n",
      "发现网格数: 12\n",
      "成功轨迹数: 0\n",
      "最佳奖励: -0.21\n",
      "专家轨迹数: 0\n",
      "--------------------------------------------------\n",
      "gg\n",
      "gg\n",
      "探索进度: 600/2000\n",
      "发现网格数: 12\n",
      "成功轨迹数: 0\n",
      "最佳奖励: 0.00\n",
      "专家轨迹数: 0\n",
      "--------------------------------------------------\n",
      "gg\n",
      "gg\n",
      "gg\n",
      "gg\n",
      "gg\n",
      "gg\n",
      "探索进度: 700/2000\n",
      "发现网格数: 12\n",
      "成功轨迹数: 0\n",
      "最佳奖励: 0.00\n",
      "专家轨迹数: 0\n",
      "--------------------------------------------------\n",
      "gg\n",
      "gg\n",
      "探索进度: 800/2000\n",
      "发现网格数: 12\n",
      "成功轨迹数: 0\n",
      "最佳奖励: 0.00\n",
      "专家轨迹数: 0\n",
      "--------------------------------------------------\n",
      "gg\n",
      "gg\n",
      "探索进度: 900/2000\n",
      "发现网格数: 12\n",
      "成功轨迹数: 0\n",
      "最佳奖励: 0.00\n",
      "专家轨迹数: 0\n",
      "--------------------------------------------------\n",
      "gg\n",
      "gg\n",
      "gg\n",
      "gg\n",
      "gg\n",
      "探索进度: 1000/2000\n",
      "发现网格数: 12\n",
      "成功轨迹数: 0\n",
      "最佳奖励: 0.00\n",
      "专家轨迹数: 0\n",
      "--------------------------------------------------\n",
      "gg\n",
      "gg\n",
      "gg\n",
      "探索进度: 1100/2000\n",
      "发现网格数: 12\n",
      "成功轨迹数: 0\n",
      "最佳奖励: 0.00\n",
      "专家轨迹数: 0\n",
      "--------------------------------------------------\n",
      "gg\n",
      "探索进度: 1200/2000\n",
      "发现网格数: 12\n",
      "成功轨迹数: 0\n",
      "最佳奖励: 0.00\n",
      "专家轨迹数: 0\n",
      "--------------------------------------------------\n",
      "gg\n",
      "探索进度: 1300/2000\n",
      "发现网格数: 12\n",
      "成功轨迹数: 0\n",
      "最佳奖励: 0.00\n",
      "专家轨迹数: 0\n",
      "--------------------------------------------------\n",
      "gg\n",
      "gg\n",
      "gg\n",
      "gg\n",
      "探索进度: 1400/2000\n",
      "发现网格数: 12\n",
      "成功轨迹数: 0\n",
      "最佳奖励: 0.00\n",
      "专家轨迹数: 0\n",
      "--------------------------------------------------\n",
      "gg\n",
      "gg\n",
      "gg\n",
      "gg\n",
      "gg\n",
      "探索进度: 1500/2000\n",
      "发现网格数: 12\n",
      "成功轨迹数: 0\n",
      "最佳奖励: 0.00\n",
      "专家轨迹数: 0\n",
      "--------------------------------------------------\n",
      "gg\n",
      "gg\n",
      "gg\n",
      "gg\n",
      "gg\n",
      "gg\n",
      "探索进度: 1600/2000\n",
      "发现网格数: 12\n",
      "成功轨迹数: 0\n",
      "最佳奖励: 0.00\n",
      "专家轨迹数: 0\n",
      "--------------------------------------------------\n",
      "gg\n",
      "探索进度: 1700/2000\n",
      "发现网格数: 12\n",
      "成功轨迹数: 0\n",
      "最佳奖励: 0.00\n",
      "专家轨迹数: 0\n",
      "--------------------------------------------------\n",
      "gg\n",
      "gg\n",
      "gg\n",
      "gg\n",
      "gg\n",
      "gg\n",
      "探索进度: 1800/2000\n",
      "发现网格数: 12\n",
      "成功轨迹数: 0\n",
      "最佳奖励: 0.00\n",
      "专家轨迹数: 0\n",
      "--------------------------------------------------\n",
      "gg\n",
      "gg\n",
      "探索进度: 1900/2000\n",
      "发现网格数: 12\n",
      "成功轨迹数: 0\n",
      "最佳奖励: 0.00\n",
      "专家轨迹数: 0\n",
      "--------------------------------------------------\n",
      "gg\n",
      "gg\n",
      "gg\n",
      "gg\n",
      "gg\n",
      "gg\n",
      "Go-Explore探索完成!\n",
      "============================================================\n",
      "Go-Explore最终统计:\n",
      "总发现网格数: 12\n",
      "成功轨迹数: 0\n",
      "最佳奖励: 0.00\n",
      "专家轨迹数: 0\n",
      "平均轨迹奖励: nan\n",
      "============================================================\n",
      "专家数据已保存到: go_explore_expert_data.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/lib/python3.12/site-packages/numpy/_core/_methods.py:145: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/var/folders/59/76h52s611154cw1jnz1dgz5r0000gn/T/ipykernel_55267/2432414582.py:326: UserWarning: Glyph 22352 (\\N{CJK UNIFIED IDEOGRAPH-5750}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/var/folders/59/76h52s611154cw1jnz1dgz5r0000gn/T/ipykernel_55267/2432414582.py:326: UserWarning: Glyph 26631 (\\N{CJK UNIFIED IDEOGRAPH-6807}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/var/folders/59/76h52s611154cw1jnz1dgz5r0000gn/T/ipykernel_55267/2432414582.py:326: UserWarning: Glyph 25506 (\\N{CJK UNIFIED IDEOGRAPH-63A2}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/var/folders/59/76h52s611154cw1jnz1dgz5r0000gn/T/ipykernel_55267/2432414582.py:326: UserWarning: Glyph 32034 (\\N{CJK UNIFIED IDEOGRAPH-7D22}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/var/folders/59/76h52s611154cw1jnz1dgz5r0000gn/T/ipykernel_55267/2432414582.py:326: UserWarning: Glyph 30340 (\\N{CJK UNIFIED IDEOGRAPH-7684}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/var/folders/59/76h52s611154cw1jnz1dgz5r0000gn/T/ipykernel_55267/2432414582.py:326: UserWarning: Glyph 29366 (\\N{CJK UNIFIED IDEOGRAPH-72B6}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/var/folders/59/76h52s611154cw1jnz1dgz5r0000gn/T/ipykernel_55267/2432414582.py:326: UserWarning: Glyph 24577 (\\N{CJK UNIFIED IDEOGRAPH-6001}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/var/folders/59/76h52s611154cw1jnz1dgz5r0000gn/T/ipykernel_55267/2432414582.py:326: UserWarning: Glyph 31354 (\\N{CJK UNIFIED IDEOGRAPH-7A7A}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/var/folders/59/76h52s611154cw1jnz1dgz5r0000gn/T/ipykernel_55267/2432414582.py:326: UserWarning: Glyph 38388 (\\N{CJK UNIFIED IDEOGRAPH-95F4}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/var/folders/59/76h52s611154cw1jnz1dgz5r0000gn/T/ipykernel_55267/2432414582.py:326: UserWarning: Glyph 26368 (\\N{CJK UNIFIED IDEOGRAPH-6700}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/var/folders/59/76h52s611154cw1jnz1dgz5r0000gn/T/ipykernel_55267/2432414582.py:326: UserWarning: Glyph 20339 (\\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/var/folders/59/76h52s611154cw1jnz1dgz5r0000gn/T/ipykernel_55267/2432414582.py:326: UserWarning: Glyph 22870 (\\N{CJK UNIFIED IDEOGRAPH-5956}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/var/folders/59/76h52s611154cw1jnz1dgz5r0000gn/T/ipykernel_55267/2432414582.py:326: UserWarning: Glyph 21169 (\\N{CJK UNIFIED IDEOGRAPH-52B1}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/var/folders/59/76h52s611154cw1jnz1dgz5r0000gn/T/ipykernel_55267/2432414582.py:326: UserWarning: Glyph 39057 (\\N{CJK UNIFIED IDEOGRAPH-9891}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/var/folders/59/76h52s611154cw1jnz1dgz5r0000gn/T/ipykernel_55267/2432414582.py:326: UserWarning: Glyph 27425 (\\N{CJK UNIFIED IDEOGRAPH-6B21}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/var/folders/59/76h52s611154cw1jnz1dgz5r0000gn/T/ipykernel_55267/2432414582.py:326: UserWarning: Glyph 20998 (\\N{CJK UNIFIED IDEOGRAPH-5206}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/var/folders/59/76h52s611154cw1jnz1dgz5r0000gn/T/ipykernel_55267/2432414582.py:326: UserWarning: Glyph 24067 (\\N{CJK UNIFIED IDEOGRAPH-5E03}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/opt/miniconda3/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 22352 (\\N{CJK UNIFIED IDEOGRAPH-5750}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/miniconda3/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 26631 (\\N{CJK UNIFIED IDEOGRAPH-6807}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/miniconda3/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 25506 (\\N{CJK UNIFIED IDEOGRAPH-63A2}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/miniconda3/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 32034 (\\N{CJK UNIFIED IDEOGRAPH-7D22}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/miniconda3/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 30340 (\\N{CJK UNIFIED IDEOGRAPH-7684}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/miniconda3/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 29366 (\\N{CJK UNIFIED IDEOGRAPH-72B6}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/miniconda3/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 24577 (\\N{CJK UNIFIED IDEOGRAPH-6001}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/miniconda3/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 31354 (\\N{CJK UNIFIED IDEOGRAPH-7A7A}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/miniconda3/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 38388 (\\N{CJK UNIFIED IDEOGRAPH-95F4}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/miniconda3/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 26368 (\\N{CJK UNIFIED IDEOGRAPH-6700}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/miniconda3/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 20339 (\\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/miniconda3/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 22870 (\\N{CJK UNIFIED IDEOGRAPH-5956}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/miniconda3/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 21169 (\\N{CJK UNIFIED IDEOGRAPH-52B1}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/miniconda3/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 39057 (\\N{CJK UNIFIED IDEOGRAPH-9891}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/miniconda3/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 27425 (\\N{CJK UNIFIED IDEOGRAPH-6B21}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/miniconda3/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 20998 (\\N{CJK UNIFIED IDEOGRAPH-5206}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/opt/miniconda3/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 24067 (\\N{CJK UNIFIED IDEOGRAPH-5E03}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('Single UAV path planning/path planning')\n",
    "\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import copy\n",
    "from collections import defaultdict, deque\n",
    "import random\n",
    "from rl_env.path_env import RlGame\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GoExploreExpert:\n",
    "    def __init__(self, env, cell_size=50, max_episodes=1000, archive_size=10000):\n",
    "        \"\"\"\n",
    "        Go-Explore专家数据生成器\n",
    "        \n",
    "        Args:\n",
    "            env: 环境实例\n",
    "            cell_size: 状态空间离散化的网格大小\n",
    "            max_episodes: 最大探索轮数\n",
    "            archive_size: 存档最大容量\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.cell_size = cell_size\n",
    "        self.max_episodes = max_episodes\n",
    "        self.archive_size = archive_size\n",
    "        \n",
    "        # 状态存档：key=cell, value=最佳轨迹信息\n",
    "        self.archive = {}\n",
    "        # 专家轨迹数据\n",
    "        self.expert_trajectories = []\n",
    "        # 探索统计\n",
    "        self.exploration_stats = {\n",
    "            'cells_discovered': 0,\n",
    "            'successful_episodes': 0,\n",
    "            'best_reward': -float('inf')\n",
    "        }\n",
    "        \n",
    "    def discretize_state(self, state):\n",
    "        \"\"\"将连续状态离散化为网格单元\"\"\"\n",
    "        x = int(state[0] * 1000 // self.cell_size)  # 位置x\n",
    "        y = int(state[1] * 1000 // self.cell_size)  # 位置y\n",
    "        # 可以添加更多维度的离散化\n",
    "        return (x, y)\n",
    "    \n",
    "    def calculate_cell_score(self, cell_info):\n",
    "        \"\"\"计算网格单元的探索价值分数\"\"\"\n",
    "        # 基础分数基于奖励\n",
    "        score = cell_info['best_reward']\n",
    "        \n",
    "        # 添加探索奖励（访问次数越少分数越高）\n",
    "        exploration_bonus = 100.0 / (cell_info['visit_count'] + 1)\n",
    "        \n",
    "        # 距离目标的奖励\n",
    "        distance_to_goal = np.sqrt((cell_info['state'][0] - cell_info['state'][4])**2 + \n",
    "                                 (cell_info['state'][1] - cell_info['state'][5])**2)\n",
    "        distance_reward = -distance_to_goal * 10\n",
    "        \n",
    "        return score + exploration_bonus + distance_reward\n",
    "    \n",
    "    def select_cell_to_explore(self):\n",
    "        \"\"\"选择要探索的网格单元\"\"\"\n",
    "        if not self.archive:\n",
    "            return None\n",
    "            \n",
    "        # 计算所有网格的探索分数\n",
    "        cell_scores = []\n",
    "        for cell, info in self.archive.items():\n",
    "            score = self.calculate_cell_score(info)\n",
    "            cell_scores.append((score, cell, info))\n",
    "        \n",
    "        # 使用softmax选择，倾向于选择高分数的网格\n",
    "        scores = np.array([score for score, _, _ in cell_scores])\n",
    "        if len(scores) > 0:\n",
    "            # 添加温度参数控制探索vs利用\n",
    "            temperature = 2.0\n",
    "            exp_scores = np.exp(scores / temperature)\n",
    "            probabilities = exp_scores / np.sum(exp_scores)\n",
    "            \n",
    "            selected_idx = np.random.choice(len(cell_scores), p=probabilities)\n",
    "            return cell_scores[selected_idx]\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def restore_to_cell(self, cell_info):\n",
    "        \"\"\"将环境恢复到指定网格单元的状态\"\"\"\n",
    "        # 这里需要根据你的环境实现状态恢复\n",
    "        # 由于path_env没有直接的状态设置方法，我们使用轨迹重放\n",
    "        trajectory = cell_info['trajectory']\n",
    "        \n",
    "        state = self.env.reset()\n",
    "        for action in trajectory:\n",
    "            state, _, done, _, _, _, _ = self.env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def explore_from_cell(self, start_state, max_steps=200):\n",
    "        \"\"\"从指定状态开始探索\"\"\"\n",
    "        trajectory = []\n",
    "        states = []\n",
    "        rewards = []\n",
    "        \n",
    "        state = start_state\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # 使用改进的动作选择策略\n",
    "            action = self.select_action(state, step, max_steps)\n",
    "            \n",
    "            next_state, reward, done, edge_r, obstacle_r, goal_r, win = self.env.step(action)\n",
    "            \n",
    "            trajectory.append(action.copy())\n",
    "            states.append(state.copy())\n",
    "            rewards.append(reward)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # 更新存档\n",
    "            cell = self.discretize_state(next_state)\n",
    "            self.update_archive(cell, next_state, trajectory.copy(), total_reward)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                if win:\n",
    "                    self.exploration_stats['successful_episodes'] += 1\n",
    "                    # 成功轨迹加入专家数据\n",
    "                    self.expert_trajectories.append({\n",
    "                        'states': states,\n",
    "                        'actions': trajectory,\n",
    "                        'rewards': rewards,\n",
    "                        'total_reward': total_reward,\n",
    "                        'success': True\n",
    "                    })\n",
    "                break\n",
    "        \n",
    "        return total_reward, len(trajectory), win\n",
    "    \n",
    "    def select_action(self, state, step, max_steps):\n",
    "        \"\"\"改进的动作选择策略\"\"\"\n",
    "        # 目标导向的启发式动作\n",
    "        goal_x, goal_y = state[4], state[5]\n",
    "        current_x, current_y = state[0], state[1]\n",
    "        \n",
    "        # 计算朝向目标的方向\n",
    "        dx = goal_x - current_x\n",
    "        dy = goal_y - current_y\n",
    "        distance = np.sqrt(dx**2 + dy**2)\n",
    "        \n",
    "        if distance > 0.01:  # 避免除零\n",
    "            # 归一化方向向量\n",
    "            target_action = np.array([dx/distance, dy/distance])\n",
    "            \n",
    "            # 添加噪声进行探索\n",
    "            noise_scale = 0.3 * (1.0 - step/max_steps)  # 随时间减少噪声\n",
    "            noise = np.random.normal(0, noise_scale, 2)\n",
    "            action = target_action + noise\n",
    "            \n",
    "            # 限制动作范围\n",
    "            action = np.clip(action, -1, 1)\n",
    "        else:\n",
    "            # 随机探索\n",
    "            action = np.random.uniform(-1, 1, 2)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def update_archive(self, cell, state, trajectory, reward):\n",
    "        \"\"\"更新存档\"\"\"\n",
    "        if cell not in self.archive:\n",
    "            self.archive[cell] = {\n",
    "                'state': state.copy(),\n",
    "                'trajectory': trajectory.copy(),\n",
    "                'best_reward': reward,\n",
    "                'visit_count': 1,\n",
    "                'first_visit_step': len(trajectory)\n",
    "            }\n",
    "            self.exploration_stats['cells_discovered'] += 1\n",
    "        else:\n",
    "            self.archive[cell]['visit_count'] += 1\n",
    "            if reward > self.archive[cell]['best_reward']:\n",
    "                self.archive[cell]['state'] = state.copy()\n",
    "                self.archive[cell]['trajectory'] = trajectory.copy()\n",
    "                self.archive[cell]['best_reward'] = reward\n",
    "        \n",
    "        # 更新全局最佳奖励\n",
    "        if reward > self.exploration_stats['best_reward']:\n",
    "            self.exploration_stats['best_reward'] = reward\n",
    "        \n",
    "        # 限制存档大小\n",
    "        if len(self.archive) > self.archive_size:\n",
    "            self.prune_archive()\n",
    "    \n",
    "    def prune_archive(self):\n",
    "        \"\"\"修剪存档，保留最有价值的网格\"\"\"\n",
    "        # 计算所有网格的分数\n",
    "        scored_cells = []\n",
    "        for cell, info in self.archive.items():\n",
    "            score = self.calculate_cell_score(info)\n",
    "            scored_cells.append((score, cell))\n",
    "        \n",
    "        # 保留分数最高的网格\n",
    "        scored_cells.sort(reverse=True)\n",
    "        keep_size = int(self.archive_size * 0.8)  # 保留80%\n",
    "        \n",
    "        new_archive = {}\n",
    "        for i in range(min(keep_size, len(scored_cells))):\n",
    "            _, cell = scored_cells[i]\n",
    "            new_archive[cell] = self.archive[cell]\n",
    "        \n",
    "        self.archive = new_archive\n",
    "    \n",
    "    def generate_expert_data(self):\n",
    "        \"\"\"生成专家数据的主循环\"\"\"\n",
    "        print(\"开始Go-Explore专家数据生成...\")\n",
    "        \n",
    "        # 第一轮：随机探索建立初始存档\n",
    "        print(\"阶段1: 初始随机探索\")\n",
    "        for episode in range(min(100, self.max_episodes // 10)):\n",
    "            state = self.env.reset()\n",
    "            self.explore_from_cell(state, max_steps=300)\n",
    "            \n",
    "            if episode % 20 == 0:\n",
    "                print(f\"初始探索进度: {episode}/100, 发现网格数: {self.exploration_stats['cells_discovered']}\")\n",
    "        \n",
    "        # 第二轮：基于存档的系统探索\n",
    "        print(\"阶段2: 基于存档的系统探索\")\n",
    "        for episode in range(100, self.max_episodes):\n",
    "            # 选择要探索的网格\n",
    "            selected = self.select_cell_to_explore()\n",
    "            \n",
    "            if selected is None:\n",
    "                # 如果没有存档，进行随机探索\n",
    "                state = self.env.reset()\n",
    "            else:\n",
    "                score, cell, cell_info = selected\n",
    "                # 恢复到选定的状态\n",
    "                state = self.restore_to_cell(cell_info)\n",
    "            \n",
    "            # 从该状态探索\n",
    "            reward, steps, win = self.explore_from_cell(state)\n",
    "            \n",
    "            if episode % 100 == 0:\n",
    "                print(f\"探索进度: {episode}/{self.max_episodes}\")\n",
    "                print(f\"发现网格数: {self.exploration_stats['cells_discovered']}\")\n",
    "                print(f\"成功轨迹数: {self.exploration_stats['successful_episodes']}\")\n",
    "                print(f\"最佳奖励: {self.exploration_stats['best_reward']:.2f}\")\n",
    "                print(f\"专家轨迹数: {len(self.expert_trajectories)}\")\n",
    "                print(\"-\" * 50)\n",
    "        \n",
    "        print(\"Go-Explore探索完成!\")\n",
    "        self.print_final_stats()\n",
    "        return self.expert_trajectories\n",
    "    \n",
    "    def print_final_stats(self):\n",
    "        \"\"\"打印最终统计信息\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"Go-Explore最终统计:\")\n",
    "        print(f\"总发现网格数: {self.exploration_stats['cells_discovered']}\")\n",
    "        print(f\"成功轨迹数: {self.exploration_stats['successful_episodes']}\")\n",
    "        print(f\"最佳奖励: {self.exploration_stats['best_reward']:.2f}\")\n",
    "        print(f\"专家轨迹数: {len(self.expert_trajectories)}\")\n",
    "        print(f\"平均轨迹奖励: {np.mean([traj['total_reward'] for traj in self.expert_trajectories]):.2f}\")\n",
    "        print(\"=\" * 60)\n",
    "    \n",
    "    def save_expert_data(self, filename='go_explore_expert_data.pkl'):\n",
    "        \"\"\"保存专家数据\"\"\"\n",
    "        expert_data = {\n",
    "            'trajectories': self.expert_trajectories,\n",
    "            'archive': self.archive,\n",
    "            'stats': self.exploration_stats\n",
    "        }\n",
    "        \n",
    "        with open(filename, 'wb') as f:\n",
    "            pkl.dump(expert_data, f, pkl.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        print(f\"专家数据已保存到: {filename}\")\n",
    "        return filename\n",
    "    \n",
    "    def visualize_exploration(self):\n",
    "        \"\"\"可视化探索结果\"\"\"\n",
    "        if not self.archive:\n",
    "            print(\"没有探索数据可视化\")\n",
    "            return\n",
    "        \n",
    "        # 提取网格位置和奖励\n",
    "        cells = list(self.archive.keys())\n",
    "        x_coords = [cell[0] * self.cell_size / 1000 for cell in cells]\n",
    "        y_coords = [cell[1] * self.cell_size / 1000 for cell in cells]\n",
    "        rewards = [info['best_reward'] for info in self.archive.values()]\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # 绘制探索的网格\n",
    "        plt.subplot(2, 2, 1)\n",
    "        scatter = plt.scatter(x_coords, y_coords, c=rewards, cmap='viridis', alpha=0.7)\n",
    "        plt.colorbar(scatter, label='最佳奖励')\n",
    "        plt.xlabel('X坐标')\n",
    "        plt.ylabel('Y坐标')\n",
    "        plt.title('Go-Explore探索的状态空间')\n",
    "        \n",
    "        # 绘制奖励分布\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.hist(rewards, bins=30, alpha=0.7)\n",
    "        plt.xlabel('奖励')\n",
    "        plt.ylabel('频次')\n",
    "        plt.title('探索奖励分布')\n",
    "        \n",
    "        # 绘制成功轨迹的奖励\n",
    "        if self.expert_trajectories:\n",
    "            success_rewards = [traj['total_reward'] for traj in self.expert_trajectories]\n",
    "            plt.subplot(2, 2, 3)\n",
    "            plt.plot(success_rewards)\n",
    "            plt.xlabel('轨迹索引')\n",
    "            plt.ylabel('总奖励')\n",
    "            plt.title('专家轨迹奖励趋势')\n",
    "            \n",
    "            # 绘制轨迹长度分布\n",
    "            trajectory_lengths = [len(traj['actions']) for traj in self.expert_trajectories]\n",
    "            plt.subplot(2, 2, 4)\n",
    "            plt.hist(trajectory_lengths, bins=20, alpha=0.7)\n",
    "            plt.xlabel('轨迹长度')\n",
    "            plt.ylabel('频次')\n",
    "            plt.title('专家轨迹长度分布')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    # 环境配置\n",
    "    N_Agent = 1\n",
    "    M_Enemy = 5\n",
    "    L_Obstacle = 3\n",
    "    RENDER = False  # 关闭渲染加速训练\n",
    "    \n",
    "    # 创建环境\n",
    "    env = RlGame(n=N_Agent, m=M_Enemy, l=L_Obstacle, render=RENDER).unwrapped\n",
    "    \n",
    "    # 创建Go-Explore专家数据生成器\n",
    "    go_explore = GoExploreExpert(\n",
    "        env=env,\n",
    "        cell_size=50,  # 网格大小\n",
    "        max_episodes=2000,  # 探索轮数\n",
    "        archive_size=5000   # 存档容量\n",
    "    )\n",
    "    \n",
    "    # 生成专家数据\n",
    "    expert_trajectories = go_explore.generate_expert_data()\n",
    "    \n",
    "    # 保存数据\n",
    "    filename = go_explore.save_expert_data('go_explore_expert_data.pkl')\n",
    "    \n",
    "    # 可视化结果\n",
    "    go_explore.visualize_exploration()\n",
    "    \n",
    "    # 分析专家数据质量\n",
    "    if expert_trajectories:\n",
    "        print(\"\\n专家数据质量分析:\")\n",
    "        rewards = [traj['total_reward'] for traj in expert_trajectories]\n",
    "        lengths = [len(traj['actions']) for traj in expert_trajectories]\n",
    "        \n",
    "        print(f\"平均奖励: {np.mean(rewards):.2f} ± {np.std(rewards):.2f}\")\n",
    "        print(f\"最高奖励: {np.max(rewards):.2f}\")\n",
    "        print(f\"平均轨迹长度: {np.mean(lengths):.1f} ± {np.std(lengths):.1f}\")\n",
    "        print(f\"成功率: {len(expert_trajectories) / go_explore.max_episodes * 100:.1f}%\")\n",
    "    \n",
    "    return expert_trajectories, filename\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    expert_data, data_file = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f856cd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功加载专家数据:\n",
      "- 专家轨迹数: 0\n",
      "- 探索网格数: 12\n",
      "- 成功轨迹数: 0\n",
      "没有专家轨迹可分析\n",
      "没有专家轨迹可可视化\n",
      "创建环境进行对比测试...\n",
      "测试随机策略性能...\n",
      "gg\n",
      "aa\n",
      "gg\n",
      "gg\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mZeroDivisionError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 423\u001b[39m\n\u001b[32m    420\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m专家数据演示完成!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 401\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    398\u001b[39m demo.visualize_all_trajectories()\n\u001b[32m    400\u001b[39m \u001b[38;5;66;03m# 与随机策略对比\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m \u001b[43mdemo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompare_with_random_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[38;5;66;03m# 询问是否进行环境演示\u001b[39;00m\n\u001b[32m    404\u001b[39m response = \u001b[38;5;28minput\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m是否在环境中演示专家轨迹? (y/n): \u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 351\u001b[39m, in \u001b[36mExpertDataDemo.compare_with_random_policy\u001b[39m\u001b[34m(self, num_episodes)\u001b[39m\n\u001b[32m    349\u001b[39m expert_rewards = [traj[\u001b[33m'\u001b[39m\u001b[33mtotal_reward\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m traj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.expert_trajectories]\n\u001b[32m    350\u001b[39m expert_lengths = [\u001b[38;5;28mlen\u001b[39m(traj[\u001b[33m'\u001b[39m\u001b[33mactions\u001b[39m\u001b[33m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m traj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.expert_trajectories]\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m expert_success_rate = \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtraj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexpert_trajectories\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtraj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msuccess\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexpert_trajectories\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# 对比结果\u001b[39;00m\n\u001b[32m    354\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n",
      "\u001b[31mZeroDivisionError\u001b[39m: division by zero"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('Single UAV path planning/path planning')\n",
    "\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from rl_env.path_env import RlGame\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.patches import Circle\n",
    "import cv2\n",
    "\n",
    "class ExpertDataDemo:\n",
    "    \"\"\"专家数据集演示器\"\"\"\n",
    "    \n",
    "    def __init__(self, expert_data_file, env=None):\n",
    "        self.expert_data_file = expert_data_file\n",
    "        self.env = env\n",
    "        self.expert_trajectories = []\n",
    "        self.load_expert_data()\n",
    "        \n",
    "    def load_expert_data(self):\n",
    "        \"\"\"加载专家数据\"\"\"\n",
    "        try:\n",
    "            with open(self.expert_data_file, 'rb') as f:\n",
    "                expert_data = pkl.load(f)\n",
    "            \n",
    "            self.expert_trajectories = expert_data['trajectories']\n",
    "            self.archive = expert_data.get('archive', {})\n",
    "            self.stats = expert_data.get('stats', {})\n",
    "            \n",
    "            print(f\"成功加载专家数据:\")\n",
    "            print(f\"- 专家轨迹数: {len(self.expert_trajectories)}\")\n",
    "            print(f\"- 探索网格数: {len(self.archive)}\")\n",
    "            print(f\"- 成功轨迹数: {self.stats.get('successful_episodes', 0)}\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"未找到专家数据文件: {self.expert_data_file}\")\n",
    "            print(\"请先运行Go-Explore算法生成专家数据\")\n",
    "        except Exception as e:\n",
    "            print(f\"加载专家数据时出错: {e}\")\n",
    "    \n",
    "    def demo_in_environment(self, num_demos=5, render_speed=0.1):\n",
    "        \"\"\"在环境中演示专家轨迹\"\"\"\n",
    "        if not self.expert_trajectories:\n",
    "            print(\"没有专家轨迹可演示\")\n",
    "            return\n",
    "        \n",
    "        if self.env is None:\n",
    "            print(\"创建演示环境...\")\n",
    "            self.env = RlGame(n=1, m=5, l=3, render=True).unwrapped\n",
    "        \n",
    "        # 选择最好的轨迹进行演示\n",
    "        best_trajectories = sorted(\n",
    "            self.expert_trajectories, \n",
    "            key=lambda x: x['total_reward'], \n",
    "            reverse=True\n",
    "        )[:num_demos]\n",
    "        \n",
    "        print(f\"演示前{num_demos}条最佳专家轨迹...\")\n",
    "        \n",
    "        for i, trajectory in enumerate(best_trajectories):\n",
    "            print(f\"\\n演示轨迹 {i+1}/{num_demos}\")\n",
    "            print(f\"奖励: {trajectory['total_reward']:.2f}\")\n",
    "            print(f\"步数: {len(trajectory['actions'])}\")\n",
    "            print(f\"成功: {'是' if trajectory.get('success', False) else '否'}\")\n",
    "            \n",
    "            # 重置环境\n",
    "            state = self.env.reset()\n",
    "            \n",
    "            # 逐步执行专家动作\n",
    "            for step, action in enumerate(trajectory['actions']):\n",
    "                # 渲染当前状态\n",
    "                if self.env.Render:\n",
    "                    self.env.render()\n",
    "                \n",
    "                # 执行动作\n",
    "                next_state, reward, done, _, _, _, win = self.env.step(action)\n",
    "                state = next_state\n",
    "                \n",
    "                # 控制演示速度\n",
    "                time.sleep(render_speed)\n",
    "                \n",
    "                if done:\n",
    "                    print(f\"轨迹在第{step+1}步结束, 成功: {win}\")\n",
    "                    break\n",
    "            \n",
    "            # 显示最终状态\n",
    "            if self.env.Render:\n",
    "                self.env.render()\n",
    "                time.sleep(2.0)  # 停留2秒显示结果\n",
    "            \n",
    "            # 询问是否继续\n",
    "            if i < len(best_trajectories) - 1:\n",
    "                input(\"按Enter键继续下一个演示...\")\n",
    "    \n",
    "    def create_trajectory_animation(self, trajectory_idx=0, save_path=None):\n",
    "        \"\"\"创建轨迹动画\"\"\"\n",
    "        if not self.expert_trajectories:\n",
    "            print(\"没有专家轨迹可动画\")\n",
    "            return\n",
    "        \n",
    "        trajectory = self.expert_trajectories[trajectory_idx]\n",
    "        states = trajectory['states']\n",
    "        actions = trajectory['actions']\n",
    "        \n",
    "        print(f\"创建轨迹动画 - 奖励: {trajectory['total_reward']:.2f}\")\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # 设置子图1 - 轨迹可视化\n",
    "        ax1.set_xlim(0, 1)\n",
    "        ax1.set_ylim(0, 1)\n",
    "        ax1.set_xlabel('X坐标 (归一化)')\n",
    "        ax1.set_ylabel('Y坐标 (归一化)')\n",
    "        ax1.set_title(f'专家轨迹演示 (奖励: {trajectory[\"total_reward\"]:.2f})')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 绘制目标和障碍物\n",
    "        if len(states) > 0:\n",
    "            goal_x, goal_y = states[0][4], states[0][5]\n",
    "            goal_circle = Circle((goal_x, goal_y), 0.04, color='green', alpha=0.5, label='目标')\n",
    "            ax1.add_patch(goal_circle)\n",
    "            \n",
    "            # 绘制完整轨迹(浅色)\n",
    "            x_coords = [s[0] for s in states]\n",
    "            y_coords = [s[1] for s in states]\n",
    "            ax1.plot(x_coords, y_coords, 'b-', alpha=0.3, linewidth=1, label='完整轨迹')\n",
    "        \n",
    "        # 动态元素\n",
    "        trajectory_line, = ax1.plot([], [], 'b-', linewidth=2, label='当前轨迹')\n",
    "        uav_point, = ax1.plot([], [], 'ro', markersize=8, label='无人机')\n",
    "        \n",
    "        ax1.legend()\n",
    "        \n",
    "        # 设置子图2 - 状态信息\n",
    "        ax2.set_xlim(0, len(states))\n",
    "        ax2.set_xlabel('时间步')\n",
    "        ax2.set_ylabel('数值')\n",
    "        ax2.set_title('状态变化')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 准备状态数据\n",
    "        speeds = [s[2] for s in states]\n",
    "        distances_to_goal = [np.sqrt((s[0]-s[4])**2 + (s[1]-s[5])**2) for s in states]\n",
    "        \n",
    "        ax2.plot(range(len(speeds)), speeds, 'g-', label='速度', alpha=0.7)\n",
    "        ax2.plot(range(len(distances_to_goal)), distances_to_goal, 'r-', label='到目标距离', alpha=0.7)\n",
    "        ax2.legend()\n",
    "        \n",
    "        # 添加当前时间步指示器\n",
    "        time_indicator = ax2.axvline(x=0, color='black', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        def animate(frame):\n",
    "            if frame >= len(states):\n",
    "                return trajectory_line, uav_point, time_indicator\n",
    "            \n",
    "            # 更新轨迹\n",
    "            x_data = [s[0] for s in states[:frame+1]]\n",
    "            y_data = [s[1] for s in states[:frame+1]]\n",
    "            trajectory_line.set_data(x_data, y_data)\n",
    "            \n",
    "            # 更新无人机位置\n",
    "            if frame < len(states):\n",
    "                uav_point.set_data([states[frame][0]], [states[frame][1]])\n",
    "            \n",
    "            # 更新时间指示器\n",
    "            time_indicator.set_xdata([frame])\n",
    "            \n",
    "            return trajectory_line, uav_point, time_indicator\n",
    "        \n",
    "        # 创建动画\n",
    "        anim = animation.FuncAnimation(\n",
    "            fig, animate, frames=len(states)+10, \n",
    "            interval=100, blit=True, repeat=True\n",
    "        )\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            print(f\"保存动画到: {save_path}\")\n",
    "            anim.save(save_path, writer='pillow', fps=10)\n",
    "        \n",
    "        plt.show()\n",
    "        return anim\n",
    "    \n",
    "    def visualize_all_trajectories(self):\n",
    "        \"\"\"可视化所有专家轨迹\"\"\"\n",
    "        if not self.expert_trajectories:\n",
    "            print(\"没有专家轨迹可可视化\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # 子图1: 所有轨迹叠加\n",
    "        ax1 = axes[0, 0]\n",
    "        ax1.set_xlim(0, 1)\n",
    "        ax1.set_ylim(0, 1)\n",
    "        ax1.set_title('所有专家轨迹叠加')\n",
    "        ax1.set_xlabel('X坐标 (归一化)')\n",
    "        ax1.set_ylabel('Y坐标 (归一化)')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 绘制目标和障碍物\n",
    "        if self.expert_trajectories:\n",
    "            first_state = self.expert_trajectories[0]['states'][0]\n",
    "            goal_x, goal_y = first_state[4], first_state[5]\n",
    "            goal_circle = Circle((goal_x, goal_y), 0.04, color='green', alpha=0.5)\n",
    "            ax1.add_patch(goal_circle)\n",
    "        \n",
    "        # 绘制所有轨迹\n",
    "        for i, traj in enumerate(self.expert_trajectories):\n",
    "            states = traj['states']\n",
    "            x_coords = [s[0] for s in states]\n",
    "            y_coords = [s[1] for s in states]\n",
    "            \n",
    "            # 根据成功与否使用不同颜色\n",
    "            color = 'blue' if traj.get('success', False) else 'red'\n",
    "            alpha = 0.6 if traj.get('success', False) else 0.3\n",
    "            \n",
    "            ax1.plot(x_coords, y_coords, color=color, alpha=alpha, linewidth=1)\n",
    "        \n",
    "        # 子图2: 奖励分布\n",
    "        ax2 = axes[0, 1]\n",
    "        rewards = [traj['total_reward'] for traj in self.expert_trajectories]\n",
    "        ax2.hist(rewards, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        ax2.set_title('专家轨迹奖励分布')\n",
    "        ax2.set_xlabel('总奖励')\n",
    "        ax2.set_ylabel('频次')\n",
    "        ax2.axvline(np.mean(rewards), color='red', linestyle='--', label=f'平均值: {np.mean(rewards):.2f}')\n",
    "        ax2.legend()\n",
    "        \n",
    "        # 子图3: 轨迹长度分布\n",
    "        ax3 = axes[1, 0]\n",
    "        lengths = [len(traj['actions']) for traj in self.expert_trajectories]\n",
    "        ax3.hist(lengths, bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "        ax3.set_title('专家轨迹长度分布')\n",
    "        ax3.set_xlabel('轨迹长度 (步数)')\n",
    "        ax3.set_ylabel('频次')\n",
    "        ax3.axvline(np.mean(lengths), color='red', linestyle='--', label=f'平均值: {np.mean(lengths):.1f}')\n",
    "        ax3.legend()\n",
    "        \n",
    "        # 子图4: 成功轨迹vs失败轨迹对比\n",
    "        ax4 = axes[1, 1]\n",
    "        success_rewards = [traj['total_reward'] for traj in self.expert_trajectories if traj.get('success', False)]\n",
    "        fail_rewards = [traj['total_reward'] for traj in self.expert_trajectories if not traj.get('success', False)]\n",
    "        \n",
    "        ax4.boxplot([success_rewards, fail_rewards], labels=['成功轨迹', '失败轨迹'])\n",
    "        ax4.set_title('成功vs失败轨迹奖励对比')\n",
    "        ax4.set_ylabel('总奖励')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def analyze_expert_strategies(self):\n",
    "        \"\"\"分析专家策略特征\"\"\"\n",
    "        if not self.expert_trajectories:\n",
    "            print(\"没有专家轨迹可分析\")\n",
    "            return\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"专家策略分析报告\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # 基本统计\n",
    "        total_trajectories = len(self.expert_trajectories)\n",
    "        successful_trajectories = sum(1 for traj in self.expert_trajectories if traj.get('success', False))\n",
    "        success_rate = successful_trajectories / total_trajectories * 100\n",
    "        \n",
    "        print(f\"总轨迹数: {total_trajectories}\")\n",
    "        print(f\"成功轨迹数: {successful_trajectories}\")\n",
    "        print(f\"成功率: {success_rate:.1f}%\")\n",
    "        \n",
    "        # 奖励分析\n",
    "        rewards = [traj['total_reward'] for traj in self.expert_trajectories]\n",
    "        print(f\"\\n奖励统计:\")\n",
    "        print(f\"- 平均奖励: {np.mean(rewards):.2f} ± {np.std(rewards):.2f}\")\n",
    "        print(f\"- 最高奖励: {np.max(rewards):.2f}\")\n",
    "        print(f\"- 最低奖励: {np.min(rewards):.2f}\")\n",
    "        \n",
    "        # 轨迹长度分析\n",
    "        lengths = [len(traj['actions']) for traj in self.expert_trajectories]\n",
    "        print(f\"\\n轨迹长度统计:\")\n",
    "        print(f\"- 平均长度: {np.mean(lengths):.1f} ± {np.std(lengths):.1f}\")\n",
    "        print(f\"- 最短轨迹: {np.min(lengths)} 步\")\n",
    "        print(f\"- 最长轨迹: {np.max(lengths)} 步\")\n",
    "        \n",
    "        # 成功轨迹特征\n",
    "        if successful_trajectories > 0:\n",
    "            success_rewards = [traj['total_reward'] for traj in self.expert_trajectories if traj.get('success', False)]\n",
    "            success_lengths = [len(traj['actions']) for traj in self.expert_trajectories if traj.get('success', False)]\n",
    "            \n",
    "            print(f\"\\n成功轨迹特征:\")\n",
    "            print(f\"- 平均奖励: {np.mean(success_rewards):.2f} ± {np.std(success_rewards):.2f}\")\n",
    "            print(f\"- 平均长度: {np.mean(success_lengths):.1f} ± {np.std(success_lengths):.1f}\")\n",
    "        \n",
    "        # 动作分析\n",
    "        all_actions = []\n",
    "        for traj in self.expert_trajectories:\n",
    "            all_actions.extend(traj['actions'])\n",
    "        \n",
    "        if all_actions:\n",
    "            all_actions = np.array(all_actions)\n",
    "            print(f\"\\n动作统计:\")\n",
    "            print(f\"- 动作维度: {all_actions.shape[1]}\")\n",
    "            print(f\"- X方向动作范围: [{np.min(all_actions[:, 0]):.3f}, {np.max(all_actions[:, 0]):.3f}]\")\n",
    "            print(f\"- Y方向动作范围: [{np.min(all_actions[:, 1]):.3f}, {np.max(all_actions[:, 1]):.3f}]\")\n",
    "            print(f\"- X方向动作均值: {np.mean(all_actions[:, 0]):.3f} ± {np.std(all_actions[:, 0]):.3f}\")\n",
    "            print(f\"- Y方向动作均值: {np.mean(all_actions[:, 1]):.3f} ± {np.std(all_actions[:, 1]):.3f}\")\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "    \n",
    "    def compare_with_random_policy(self, num_episodes=20):\n",
    "        \"\"\"与随机策略对比\"\"\"\n",
    "        if self.env is None:\n",
    "            print(\"创建环境进行对比测试...\")\n",
    "            self.env = RlGame(n=1, m=5, l=3, render=False).unwrapped\n",
    "        \n",
    "        print(\"测试随机策略性能...\")\n",
    "        \n",
    "        random_rewards = []\n",
    "        random_lengths = []\n",
    "        random_success = 0\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            total_reward = 0\n",
    "            steps = 0\n",
    "            \n",
    "            for step in range(1000):  # 最大步数限制\n",
    "                # 随机动作\n",
    "                action = self.env.action_space.sample()\n",
    "                \n",
    "                next_state, reward, done, _, _, _, win = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "                steps += 1\n",
    "                \n",
    "                if done:\n",
    "                    if win:\n",
    "                        random_success += 1\n",
    "                    break\n",
    "            \n",
    "            random_rewards.append(total_reward)\n",
    "            random_lengths.append(steps)\n",
    "        \n",
    "        # 专家策略统计\n",
    "        expert_rewards = [traj['total_reward'] for traj in self.expert_trajectories]\n",
    "        expert_lengths = [len(traj['actions']) for traj in self.expert_trajectories]\n",
    "        expert_success_rate = sum(1 for traj in self.expert_trajectories if traj.get('success', False)) / len(self.expert_trajectories)\n",
    "        \n",
    "        # 对比结果\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"专家策略 vs 随机策略对比\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"{'指标':<15} {'专家策略':<20} {'随机策略':<20} {'提升':<15}\")\n",
    "        print(\"-\"*70)\n",
    "        \n",
    "        # 成功率对比\n",
    "        random_success_rate = random_success / num_episodes\n",
    "        success_improvement = (expert_success_rate - random_success_rate) / random_success_rate * 100 if random_success_rate > 0 else float('inf')\n",
    "        print(f\"{'成功率':<15} {expert_success_rate*100:<20.1f}% {random_success_rate*100:<20.1f}% {success_improvement:<15.1f}%\")\n",
    "        \n",
    "        # 平均奖励对比\n",
    "        expert_mean_reward = np.mean(expert_rewards)\n",
    "        random_mean_reward = np.mean(random_rewards)\n",
    "        reward_improvement = (expert_mean_reward - random_mean_reward) / abs(random_mean_reward) * 100 if random_mean_reward != 0 else float('inf')\n",
    "        print(f\"{'平均奖励':<15} {expert_mean_reward:<20.2f} {random_mean_reward:<20.2f} {reward_improvement:<15.1f}%\")\n",
    "        \n",
    "        # 平均步数对比\n",
    "        expert_mean_length = np.mean(expert_lengths)\n",
    "        random_mean_length = np.mean(random_lengths)\n",
    "        length_improvement = (random_mean_length - expert_mean_length) / random_mean_length * 100 if random_mean_length > 0 else 0\n",
    "        print(f\"{'平均步数':<15} {expert_mean_length:<20.1f} {random_mean_length:<20.1f} {length_improvement:<15.1f}%\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "\n",
    "def main():\n",
    "    \"\"\"主演示函数\"\"\"\n",
    "    expert_data_file = 'go_explore_expert_data.pkl'\n",
    "    \n",
    "    # 检查文件是否存在\n",
    "    import os\n",
    "    if not os.path.exists(expert_data_file):\n",
    "        print(f\"专家数据文件 {expert_data_file} 不存在!\")\n",
    "        print(\"请先运行Go-Explore算法生成专家数据\")\n",
    "        return\n",
    "    \n",
    "    # 创建演示器\n",
    "    demo = ExpertDataDemo(expert_data_file)\n",
    "    \n",
    "    # 分析专家策略\n",
    "    demo.analyze_expert_strategies()\n",
    "    \n",
    "    # 可视化所有轨迹\n",
    "    demo.visualize_all_trajectories()\n",
    "    \n",
    "    # 与随机策略对比\n",
    "    demo.compare_with_random_policy()\n",
    "    \n",
    "    # 询问是否进行环境演示\n",
    "    response = input(\"\\n是否在环境中演示专家轨迹? (y/n): \")\n",
    "    if response.lower() == 'y':\n",
    "        # 环境演示\n",
    "        demo.demo_in_environment(num_demos=3, render_speed=0.1)\n",
    "    \n",
    "    # 询问是否创建动画\n",
    "    response = input(\"\\n是否创建轨迹动画? (y/n): \")\n",
    "    if response.lower() == 'y':\n",
    "        # 创建最佳轨迹的动画\n",
    "        if demo.expert_trajectories:\n",
    "            best_idx = np.argmax([traj['total_reward'] for traj in demo.expert_trajectories])\n",
    "            demo.create_trajectory_animation(\n",
    "                trajectory_idx=best_idx,\n",
    "                save_path='best_expert_trajectory.gif'\n",
    "            )\n",
    "    \n",
    "    print(\"\\n专家数据演示完成!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396afb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cpu\n",
      "开始行为克隆训练...\n",
      "加载专家数据: go_explore_expert_data.pkl\n",
      "加载了 0 条专家轨迹\n",
      "总共提取了 0 个状态-动作对\n",
      "状态维度: (0,), 动作维度: (0,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 393\u001b[39m\n\u001b[32m    390\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m bc_trainer, results\n\u001b[32m    392\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m     trainer, eval_results = \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 365\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    364\u001b[39m \u001b[38;5;66;03m# 开始训练\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m365\u001b[39m model_path = \u001b[43mbc_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpert_data_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbc_policy_best.pth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m    370\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[38;5;66;03m# 绘制训练历史\u001b[39;00m\n\u001b[32m    373\u001b[39m bc_trainer.plot_training_history()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 209\u001b[39m, in \u001b[36mBCTrainer.train\u001b[39m\u001b[34m(self, data_file, epochs, batch_size, save_path)\u001b[39m\n\u001b[32m    206\u001b[39m states, actions = \u001b[38;5;28mself\u001b[39m.load_expert_data(data_file)\n\u001b[32m    208\u001b[39m \u001b[38;5;66;03m# 创建数据加载器\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m train_loader, val_loader = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_dataloaders\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m best_val_loss = \u001b[38;5;28mfloat\u001b[39m(\u001b[33m'\u001b[39m\u001b[33minf\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    214\u001b[39m patience = \u001b[32m20\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 138\u001b[39m, in \u001b[36mBCTrainer.create_dataloaders\u001b[39m\u001b[34m(self, states, actions, batch_size, val_split)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"创建训练和验证数据加载器\"\"\"\u001b[39;00m\n\u001b[32m    137\u001b[39m \u001b[38;5;66;03m# 分割训练和验证集\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m X_train, X_val, y_train, y_val = \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\n\u001b[32m    140\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m训练集大小: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_train)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, 验证集大小: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_val)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    144\u001b[39m \u001b[38;5;66;03m# 创建数据集\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/sklearn/model_selection/_split.py:2919\u001b[39m, in \u001b[36mtrain_test_split\u001b[39m\u001b[34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[39m\n\u001b[32m   2916\u001b[39m arrays = indexable(*arrays)\n\u001b[32m   2918\u001b[39m n_samples = _num_samples(arrays[\u001b[32m0\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m2919\u001b[39m n_train, n_test = \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2920\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.25\u001b[39;49m\n\u001b[32m   2921\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2923\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m   2924\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/sklearn/model_selection/_split.py:2499\u001b[39m, in \u001b[36m_validate_shuffle_split\u001b[39m\u001b[34m(n_samples, test_size, train_size, default_test_size)\u001b[39m\n\u001b[32m   2496\u001b[39m n_train, n_test = \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[32m   2498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_train == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2499\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2500\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2501\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2502\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33maforementioned parameters.\u001b[39m\u001b[33m\"\u001b[39m.format(n_samples, test_size, train_size)\n\u001b[32m   2503\u001b[39m     )\n\u001b[32m   2505\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[31mValueError\u001b[39m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('Single UAV path planning/path planning')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from rl_env.path_env import RlGame\n",
    "\n",
    "class ExpertDataset(Dataset):\n",
    "    \"\"\"专家数据集类\"\"\"\n",
    "    def __init__(self, states, actions):\n",
    "        self.states = torch.FloatTensor(states)\n",
    "        self.actions = torch.FloatTensor(actions)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.states[idx], self.actions[idx]\n",
    "\n",
    "class BCPolicy(nn.Module):\n",
    "    \"\"\"行为克隆策略网络\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dims=[256, 256], activation='relu'):\n",
    "        super(BCPolicy, self).__init__()\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # 选择激活函数\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        else:\n",
    "            self.activation = nn.ReLU()\n",
    "        \n",
    "        # 构建网络层\n",
    "        layers = []\n",
    "        input_dim = state_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                self.activation,\n",
    "                nn.Dropout(0.1)  # 添加dropout防止过拟合\n",
    "            ])\n",
    "            input_dim = hidden_dim\n",
    "        \n",
    "        # 输出层\n",
    "        layers.append(nn.Linear(input_dim, action_dim))\n",
    "        layers.append(nn.Tanh())  # 动作范围限制在[-1, 1]\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # 权重初始化\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"初始化网络权重\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.network(state)\n",
    "    \n",
    "    def get_action(self, state, deterministic=True):\n",
    "        \"\"\"获取动作\"\"\"\n",
    "        with torch.no_grad():\n",
    "            if isinstance(state, np.ndarray):\n",
    "                state = torch.FloatTensor(state).unsqueeze(0)\n",
    "            action = self.forward(state)\n",
    "            return action.cpu().numpy().flatten()\n",
    "\n",
    "class BCTrainer:\n",
    "    \"\"\"行为克隆训练器\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, learning_rate=1e-3, device='cpu'):\n",
    "        self.device = device\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # 创建策略网络\n",
    "        self.policy = BCPolicy(state_dim, action_dim).to(device)\n",
    "        \n",
    "        # 优化器和损失函数\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "        # 学习率调度器\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=100, gamma=0.9)\n",
    "        \n",
    "        # 训练历史\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "    def load_expert_data(self, data_file):\n",
    "        \"\"\"加载专家数据\"\"\"\n",
    "        print(f\"加载专家数据: {data_file}\")\n",
    "        \n",
    "        with open(data_file, 'rb') as f:\n",
    "            expert_data = pkl.load(f)\n",
    "        \n",
    "        trajectories = expert_data['trajectories']\n",
    "        print(f\"加载了 {len(trajectories)} 条专家轨迹\")\n",
    "        \n",
    "        # 提取状态和动作\n",
    "        all_states = []\n",
    "        all_actions = []\n",
    "        \n",
    "        for traj in trajectories:\n",
    "            states = traj['states']\n",
    "            actions = traj['actions']\n",
    "            \n",
    "            # 确保数据格式正确\n",
    "            for i in range(len(states)):\n",
    "                if i < len(actions):  # 确保状态和动作对应\n",
    "                    all_states.append(states[i])\n",
    "                    all_actions.append(actions[i])\n",
    "        \n",
    "        states = np.array(all_states)\n",
    "        actions = np.array(all_actions)\n",
    "        \n",
    "        print(f\"总共提取了 {len(states)} 个状态-动作对\")\n",
    "        print(f\"状态维度: {states.shape}, 动作维度: {actions.shape}\")\n",
    "        \n",
    "        return states, actions\n",
    "    \n",
    "    def create_dataloaders(self, states, actions, batch_size=128, val_split=0.2):\n",
    "        \"\"\"创建训练和验证数据加载器\"\"\"\n",
    "        # 分割训练和验证集\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            states, actions, test_size=val_split, random_state=42\n",
    "        )\n",
    "        \n",
    "        print(f\"训练集大小: {len(X_train)}, 验证集大小: {len(X_val)}\")\n",
    "        \n",
    "        # 创建数据集\n",
    "        train_dataset = ExpertDataset(X_train, y_train)\n",
    "        val_dataset = ExpertDataset(X_val, y_val)\n",
    "        \n",
    "        # 创建数据加载器\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        return train_loader, val_loader\n",
    "    \n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"训练一个epoch\"\"\"\n",
    "        self.policy.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for states, actions in train_loader:\n",
    "            states = states.to(self.device)\n",
    "            actions = actions.to(self.device)\n",
    "            \n",
    "            # 前向传播\n",
    "            predicted_actions = self.policy(states)\n",
    "            loss = self.criterion(predicted_actions, actions)\n",
    "            \n",
    "            # 反向传播\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # 梯度裁剪\n",
    "            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), max_norm=1.0)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        return total_loss / num_batches\n",
    "    \n",
    "    def validate(self, val_loader):\n",
    "        \"\"\"验证模型\"\"\"\n",
    "        self.policy.eval()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for states, actions in val_loader:\n",
    "                states = states.to(self.device)\n",
    "                actions = actions.to(self.device)\n",
    "                \n",
    "                predicted_actions = self.policy(states)\n",
    "                loss = self.criterion(predicted_actions, actions)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "        \n",
    "        return total_loss / num_batches\n",
    "    \n",
    "    def train(self, data_file, epochs=200, batch_size=128, save_path='bc_policy.pth'):\n",
    "        \"\"\"完整训练流程\"\"\"\n",
    "        print(\"开始行为克隆训练...\")\n",
    "        \n",
    "        # 加载数据\n",
    "        states, actions = self.load_expert_data(data_file)\n",
    "        \n",
    "        # 创建数据加载器\n",
    "        train_loader, val_loader = self.create_dataloaders(\n",
    "            states, actions, batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience = 20\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # 训练\n",
    "            train_loss = self.train_epoch(train_loader)\n",
    "            \n",
    "            # 验证\n",
    "            val_loss = self.validate(val_loader)\n",
    "            \n",
    "            # 更新学习率\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # 记录损失\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "            \n",
    "            # 早停检查\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                # 保存最佳模型\n",
    "                torch.save(self.policy.state_dict(), save_path)\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # 打印进度\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}/{epochs}\")\n",
    "                print(f\"训练损失: {train_loss:.6f}, 验证损失: {val_loss:.6f}\")\n",
    "                print(f\"学习率: {self.optimizer.param_groups[0]['lr']:.2e}\")\n",
    "                print(\"-\" * 50)\n",
    "            \n",
    "            # 早停\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"验证损失在 {patience} 个epoch内没有改善，早停训练\")\n",
    "                break\n",
    "        \n",
    "        print(f\"训练完成! 最佳模型已保存到: {save_path}\")\n",
    "        return save_path\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"绘制训练历史\"\"\"\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.train_losses, label='训练损失')\n",
    "        plt.plot(self.val_losses, label='验证损失')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('损失')\n",
    "        plt.legend()\n",
    "        plt.title('训练历史')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.train_losses[-50:], label='训练损失(最近50轮)')\n",
    "        plt.plot(self.val_losses[-50:], label='验证损失(最近50轮)')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('损失')\n",
    "        plt.legend()\n",
    "        plt.title('最近训练历史')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def evaluate_bc_policy(policy_path, env, num_episodes=50):\n",
    "    \"\"\"评估BC策略性能\"\"\"\n",
    "    # 加载训练好的策略\n",
    "    state_dim = 7  # 根据环境状态维度\n",
    "    action_dim = 2  # 根据环境动作维度\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    policy = BCPolicy(state_dim, action_dim).to(device)\n",
    "    policy.load_state_dict(torch.load(policy_path, map_location=device))\n",
    "    policy.eval()\n",
    "    \n",
    "    print(f\"评估BC策略性能 (共{num_episodes}轮)...\")\n",
    "    \n",
    "    win_times = 0\n",
    "    total_rewards = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        for step in range(1000):  # 最大步数限制\n",
    "            # 获取动作\n",
    "            action = policy.get_action(state, deterministic=True)\n",
    "            \n",
    "            # 执行动作\n",
    "            next_state, reward, done, _, _, _, win = env.step(action)\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                if win:\n",
    "                    win_times += 1\n",
    "                break\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        \n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode}: 奖励={total_reward:.2f}, 步数={steps}\")\n",
    "    \n",
    "    # 打印评估结果\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"BC策略评估结果:\")\n",
    "    print(f\"成功率: {win_times/num_episodes*100:.1f}%\")\n",
    "    print(f\"平均奖励: {np.mean(total_rewards):.2f} ± {np.std(total_rewards):.2f}\")\n",
    "    print(f\"平均步数: {np.mean(episode_lengths):.1f} ± {np.std(episode_lengths):.1f}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return {\n",
    "        'success_rate': win_times/num_episodes,\n",
    "        'mean_reward': np.mean(total_rewards),\n",
    "        'std_reward': np.std(total_rewards),\n",
    "        'mean_steps': np.mean(episode_lengths),\n",
    "        'std_steps': np.std(episode_lengths)\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    # 设置设备\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"使用设备: {device}\")\n",
    "    \n",
    "    # 环境参数\n",
    "    state_dim = 7  # [x, y, speed, theta, goal_x, goal_y, obstacle_flag]\n",
    "    action_dim = 2  # [dx, dy]\n",
    "    \n",
    "    # 创建BC训练器\n",
    "    bc_trainer = BCTrainer(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        learning_rate=1e-3,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # 训练BC策略\n",
    "    expert_data_file = 'go_explore_expert_data.pkl'\n",
    "    \n",
    "    if not os.path.exists(expert_data_file):\n",
    "        print(f\"专家数据文件 {expert_data_file} 不存在!\")\n",
    "        print(\"请先运行Go-Explore算法生成专家数据\")\n",
    "        return\n",
    "    \n",
    "    # 开始训练\n",
    "    model_path = bc_trainer.train(\n",
    "        data_file=expert_data_file,\n",
    "        epochs=300,\n",
    "        batch_size=128,\n",
    "        save_path='bc_policy_best.pth'\n",
    "    )\n",
    "    \n",
    "    # 绘制训练历史\n",
    "    bc_trainer.plot_training_history()\n",
    "    \n",
    "    # 评估训练好的策略\n",
    "    print(\"\\n开始评估BC策略...\")\n",
    "    env = RlGame(n=1, m=5, l=3, render=False).unwrapped\n",
    "    \n",
    "    results = evaluate_bc_policy(\n",
    "        policy_path=model_path,\n",
    "        env=env,\n",
    "        num_episodes=100\n",
    "    )\n",
    "    \n",
    "    # 保存评估结果\n",
    "    with open('bc_evaluation_results.pkl', 'wb') as f:\n",
    "        pkl.dump(results, f)\n",
    "    \n",
    "    print(\"BC训练和评估完成!\")\n",
    "    return bc_trainer, results\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    trainer, eval_results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0395c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
